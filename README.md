# Uber Fare Prediction â€“ Milestone 3

## ğŸ“„ Abstract
For Milestone 3, we extend our work on the **Uber NYC For-Hire Vehicles Trip Data (2024â€“2025)** by moving from exploratory analysis to **predictive modeling**.  
We focus on building a **GPU-accelerated Decision Tree Regression model (XGBoost)** to predict **base passenger fare** using trip and contextual features.  

This milestone highlights **feature engineering, hyperparameter tuning, and model performance evaluation** on a large-scale dataset.

---

## ğŸ“Œ Dataset
- **Source:** [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)  
- **Subset Used for Milestone 3:** `fhvhv_tripdata_2024-07.parquet`  
- **Size:** ~14,328,242 rows Ã— 53 features  
- **Scope:** High Volume For-Hire Service (Uber) trips in NYC  

### Features
Engineered features included:
- Temporal: `request_hour`, `request_weekday`, `pickup_hour`, `pickup_weekday`  
- Derived: `wait_time_seconds`, rush-hour flags, weekend flag  
- Distance & Duration categories  
- Top 20 pickup/dropoff location one-hot features  

**Target Variable:**  
- `base_passenger_fare` (continuous)

---

## âš™ï¸ Pre-Processing
Steps included:
1. **Filtering** invalid zones (`PULocationID=264`, `DOLocationID=265`).  
2. **Feature engineering** for temporal, categorical, and ride-based patterns.  
3. **One-hot encoding** top pickup and dropoff locations.  
4. Train/validation/test split (80/10/10).  

---

## ğŸš€ Modeling

We used **XGBoost Regressor with GPU acceleration** (`tree_method='hist', device='cuda:0'`).  
Hyperparameter tuning focused on the **max_depth** parameter, which controls tree complexity.  

### Depth Search Strategy
- Originally tested **depths 1 â†’ 20**.  
- Found best models consistently in **10 â†’ 15** range.  
- To save compute time, narrowed tuning to that range.  
- Final best depth was **12**.  

---

## ğŸ“Š Results

### 1. Validation Error vs. Depth
We tested multiple depths, finding that validation error minimized at **depth=12**.  

<img width="1600" height="1000" alt="val_mse_vs_max_depth" src="https://github.com/user-attachments/assets/9a0497f4-6856-4b5a-adc7-51136d24b2cb" />
  
*(Generated from July 2024 dataset, ~14.3M rows)*

---

### 2. Shallow vs. Best Depth Comparison
We compared a shallow tree (`d=2`) vs. the best depth (`d=12`).  
The deeper model showed consistently lower RMSE across training, validation, and testing.  

<img width="780" height="498" alt="image" src="https://github.com/user-attachments/assets/3f58ee4a-e61b-448b-9203-f44f36c08770" />

---

### Final Model Performance (Best depth = 12)
- **Test MSE:** ~96.62  
- **Test RMSE:** ~$9.83  
- **Most important features:**  
  - `distance_category`, `trip_miles`, `trip_time`, duration-related features, and specific pickup/dropoff zones.  

---

## ğŸ“ˆ Key Takeaways
- Distance-based features dominate fare prediction.  
- Temporal features (hour, weekday, rush-hour flags) and pickup/dropoff zones also play a key role.  
- Narrowing the depth range (10â€“15) significantly reduced tuning time without sacrificing accuracy.  

---

## ğŸ” Model Fit Analysis
We originally tested **max_depth values from 1 to 20**, which gave a clear picture of underfitting vs. overfitting:  
- Depths 1â€“5: **Underfit** (high bias, high error).  
- Depths 16â€“20: **Overfit** (validation error increasing).  
- Depths 10â€“15: **Best balance**.  

â¡ï¸ The chosen model at depth 12 sits in the **sweet spot** of the fitting curve: not underfit, not overfit.  

â¡ï¸ **Next models to try:**  
- **SVMs** â†’ margin-based performance, robustness to outliers.  
- **KNNs** â†’ distance-based regression comparison.  
- **Naive Bayes** â†’ lightweight categorical baseline.  

---

## ğŸ“ Conclusion
Our first decision tree model (XGBoost, GPU) achieved:  
- **Best depth:** 12  
- **Final Test RMSE:** ~$9.83  
- **Key drivers:** distance category, trip miles, and trip time.  

This confirms that **distance and geography dominate fare prediction**, while temporal features play a smaller role.  

â¡ï¸ **Future improvements:**  
- Add **regularization** to reduce overfitting.  
- Bring in **external data** (weather, traffic, surge pricing).  
- Try **ensembles** (Random Forest, Gradient Boosting, stacking).  

This model establishes a strong baseline for fare prediction and prepares us for deeper experimentation in Milestone 4.  

---

## ğŸ“‚ Code Files
- [`decisiontree-4.py`](https://github.com/annajliu110101/Uber-Driver-Pay-Algorithm/blob/Milestone3/decisiontree-4.py)  
- [`pre_processing-2.py`](https://github.com/annajliu110101/Uber-Driver-Pay-Algorithm/blob/Milestone3/pre_processing-2.py)  

---

## ğŸ”§ Environment Setup
To reproduce:
```bash
git clone https://github.com/annajliu110101/Uber-Driver-Pay-Algorithm.git
cd Uber-Driver-Pay-Algorithm
pip install -r requirements.txt
